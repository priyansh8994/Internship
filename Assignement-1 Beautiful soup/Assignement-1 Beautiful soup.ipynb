{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d1f16f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Headers\n",
      "0  Wikipedia\\n\\nThe Free Encyclopedia\n",
      "1            1 000 000+\\n\\n\\narticles\n",
      "2              100 000+\\n\\n\\narticles\n",
      "3               10 000+\\n\\n\\narticles\n",
      "4                1 000+\\n\\n\\narticles\n",
      "5                  100+\\n\\n\\narticles\n"
     ]
    }
   ],
   "source": [
    "#1st program\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = 'https://www.wikipedia.org'\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "headers = []\n",
    "for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "    headers.append(header.text.strip())\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Headers': headers})\n",
    "\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a4a648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Name Rating  Year\n",
      "0    The Shawshank Redemption    9.2  1994\n",
      "1               The Godfather    9.2  1972\n",
      "2             The Dark Knight    9.0  2008\n",
      "3       The Godfather Part II    9.0  1974\n",
      "4                12 Angry Men    9.0  1957\n",
      "..                        ...    ...   ...\n",
      "245            The Iron Giant    8.0  1999\n",
      "246                  The Help    8.0  2011\n",
      "247                   Aladdin    8.0  1992\n",
      "248               Dersu Uzala    8.0  1975\n",
      "249        Dances with Wolves    8.0  1990\n",
      "\n",
      "[250 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "movie_data = []\n",
    "movie_table = soup.find('tbody', {'class': 'lister-list'})\n",
    "for row in movie_table.find_all('tr'):\n",
    "    name = row.find('td', {'class': 'titleColumn'}).find('a').text\n",
    "    year = row.find('span', {'class': 'secondaryInfo'}).text.strip('()')\n",
    "    rating = row.find('td', {'class': 'ratingColumn'}).find('strong').text\n",
    "    movie_data.append([name, rating, year])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(movie_data, columns=['Name', 'Rating', 'Year'])\n",
    "\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b46e9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Name Rating  Year\n",
      "0    Ramayana: The Legend of Prince Rama    8.6  1993\n",
      "1             Rocketry: The Nambi Effect    8.4  2022\n",
      "2                                Nayakan    8.4  1987\n",
      "3                               Gol Maal    8.4  1979\n",
      "4                            777 Charlie    8.4  2022\n",
      "..                                   ...    ...   ...\n",
      "245                       Poove Unakkaga    7.7  1996\n",
      "246                                  Dia    7.7  2020\n",
      "247                    Thiruchitrambalam    7.7  2022\n",
      "248                        Minnal Murali    7.7  2021\n",
      "249                           Goodachari    7.7  2018\n",
      "\n",
      "[250 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.imdb.com/india/top-rated-indian-movies/'\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "movie_data = []\n",
    "movie_table = soup.find('tbody', {'class': 'lister-list'})\n",
    "for row in movie_table.find_all('tr'):\n",
    "    name = row.find('td', {'class': 'titleColumn'}).find('a').text\n",
    "    year = row.find('span', {'class': 'secondaryInfo'}).text.strip('()')\n",
    "    rating = row.find('td', {'class': 'ratingColumn'}).find('strong').text\n",
    "    movie_data.append([name, rating, year])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(movie_data, columns=['Name', 'Rating', 'Year'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f02bfbcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14468\\2941711235.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mpresident_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'tablepress'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "president_data = []\n",
    "table = soup.find('table', {'class': 'tablepress'})\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    name = row.find_all('td')[0].text.strip()\n",
    "    term = row.find_all('td')[1].text.strip()\n",
    "    president_data.append([name, term])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(president_data, columns=['Name', 'Term of office'])\n",
    "\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e48a162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams:\n",
      "                Team Matches Points Rating\n",
      "0    Australia\\nAUS      35  3,965    113\n",
      "1   New Zealand\\nNZ      31  3,504    113\n",
      "2        India\\nIND      47  5,294    113\n",
      "3      England\\nENG      36  3,988    111\n",
      "4     Pakistan\\nPAK      25  2,649    106\n",
      "5  South Africa\\nSA      31  3,141    101\n",
      "6   Bangladesh\\nBAN      38  3,625     95\n",
      "7     Sri Lanka\\nSL      36  3,099     86\n",
      "8   West Indies\\nWI      43  3,105     72\n",
      "9  Afghanistan\\nAFG      20  1,419     71\n",
      "\n",
      "Top 10 ODI batsmen:\n",
      "                  Batsman Team Rating\n",
      "0             Babar Azam  PAK    887\n",
      "1  Rassie van der Dussen   SA    777\n",
      "2            Imam-ul-Haq  PAK    740\n",
      "3           Shubman Gill  IND    738\n",
      "4           David Warner  AUS    726\n",
      "5            Virat Kohli  IND    719\n",
      "6        Quinton de Kock   SA    718\n",
      "7           Rohit Sharma  IND    707\n",
      "8            Steve Smith  AUS    702\n",
      "9           Fakhar Zaman  PAK    699\n",
      "\n",
      "Top 10 ODI bowlers:\n",
      "              Bowler Team Rating\n",
      "0    Josh Hazlewood  AUS    705\n",
      "1       Trent Boult   NZ    694\n",
      "2    Mohammed Siraj  IND    691\n",
      "3    Mitchell Starc  AUS    686\n",
      "4        Matt Henry   NZ    676\n",
      "5       Rashid Khan  AFG    659\n",
      "6        Adam Zampa  AUS    652\n",
      "7    Shaheen Afridi  PAK    641\n",
      "8  Mujeeb Ur Rahman  AFG    637\n",
      "9   Shakib Al Hasan  BAN    636\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "teams_url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "teams_response = requests.get(teams_url)\n",
    "teams_soup = BeautifulSoup(teams_response.content, 'html.parser')\n",
    "\n",
    "teams_data = []\n",
    "teams_table = teams_soup.find('table', {'class': 'table'})\n",
    "for row in teams_table.find_all('tr')[1:11]:\n",
    "    cols = row.find_all('td')\n",
    "    team = cols[1].text.strip()\n",
    "    matches = cols[2].text.strip()\n",
    "    points = cols[3].text.strip()\n",
    "    rating = cols[4].text.strip()\n",
    "    teams_data.append([team, matches, points, rating])\n",
    "\n",
    "teams_df = pd.DataFrame(teams_data, columns=['Team', 'Matches', 'Points', 'Rating'])\n",
    "\n",
    "print(\"Top 10 ODI teams:\\n\", teams_df)\n",
    "\n",
    "\n",
    "batsmen_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "batsmen_response = requests.get(batsmen_url)\n",
    "batsmen_soup = BeautifulSoup(batsmen_response.content, 'html.parser')\n",
    "\n",
    "batsmen_data = []\n",
    "batsmen_table = batsmen_soup.find('table', {'class': 'table rankings-table'})\n",
    "for row in batsmen_table.find_all('tr')[1:11]:\n",
    "    cols = row.find_all('td')\n",
    "    player = cols[1].text.strip()\n",
    "    team = cols[2].text.strip()\n",
    "    rating = cols[3].text.strip()\n",
    "    batsmen_data.append([player, team, rating])\n",
    "\n",
    "batsmen_df = pd.DataFrame(batsmen_data, columns=['Batsman', 'Team', 'Rating'])\n",
    "\n",
    "print(\"\\nTop 10 ODI batsmen:\\n\", batsmen_df)\n",
    "\n",
    "\n",
    "bowlers_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "bowlers_response = requests.get(bowlers_url)\n",
    "bowlers_soup = BeautifulSoup(bowlers_response.content, 'html.parser')\n",
    "\n",
    "bowlers_data = []\n",
    "bowlers_table = bowlers_soup.find('table', {'class': 'table rankings-table'})\n",
    "for row in bowlers_table.find_all('tr')[1:11]:\n",
    "    cols = row.find_all('td')\n",
    "    player = cols[1].text.strip()\n",
    "    team = cols[2].text.strip()\n",
    "    rating = cols[3].text.strip()\n",
    "    bowlers_data.append([player, team, rating])\n",
    "\n",
    "bowlers_df = pd.DataFrame(bowlers_data, columns=['Bowler', 'Team', 'Rating'])\n",
    "\n",
    "print(\"\\nTop 10 ODI bowlers:\\n\", bowlers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf1e0736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "articles = soup.find_all('div', {'class': 'pod-listing-header'})\n",
    "\n",
    "title_list = []\n",
    "author_list = []\n",
    "date_list = []\n",
    "url_list = []\n",
    "\n",
    "for article in articles:\n",
    "    title = article.find('a').text.strip()\n",
    "    author = article.find('div', {'class': 'text-xs'}).text.strip()\n",
    "    date = article.find('div', {'class': 'pod-listing-header__details'}).text.strip().split('|')[0].strip()\n",
    "    url = article.find('a')['href']\n",
    "    title_list.append(title)\n",
    "    author_list.append(author)\n",
    "    date_list.append(date)\n",
    "    url_list.append(url)\n",
    "\n",
    "data = {\n",
    "    'Paper Title': title_list,\n",
    "    'Authors': author_list,\n",
    "    'Published Date': date_list,\n",
    "    'Paper URL': url_list\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35a47606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "               Team           Matches Points Rating\n",
      "0   New Zealand\\nNZ   New Zealand\\nNZ     31  3,504\n",
      "1        India\\nIND        India\\nIND     47  5,294\n",
      "2      England\\nENG      England\\nENG     36  3,988\n",
      "3     Pakistan\\nPAK     Pakistan\\nPAK     25  2,649\n",
      "4  South Africa\\nSA  South Africa\\nSA     31  3,141\n",
      "5   Bangladesh\\nBAN   Bangladesh\\nBAN     38  3,625\n",
      "6     Sri Lanka\\nSL     Sri Lanka\\nSL     36  3,099\n",
      "7   West Indies\\nWI   West Indies\\nWI     43  3,105\n",
      "8  Afghanistan\\nAFG  Afghanistan\\nAFG     20  1,419\n",
      "9      Ireland\\nIRE      Ireland\\nIRE     27  1,384\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Scrape top 10 ODI teams\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "teams_table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "team_name_list = []\n",
    "team_matches_list = []\n",
    "team_points_list = []\n",
    "team_rating_list = []\n",
    "\n",
    "for row in teams_table.find_all('tr', {'class': 'table-body'}):\n",
    "    # Extract team name\n",
    "    team_name = row.find('td', {'class': 'rankings-table__team'}).text.strip()\n",
    "    team_name_list.append(team_name)\n",
    "\n",
    "    # Extract team matches, points, and rating\n",
    "    row_cells = row.find_all('td', {'class': 'table-body__cell'})\n",
    "    if len(row_cells) >= 4:\n",
    "        team_matches = row_cells[1].text.strip()\n",
    "        team_points = row_cells[2].text.strip()\n",
    "        team_rating = row_cells[3].text.strip()\n",
    "        team_matches_list.append(team_matches)\n",
    "        team_points_list.append(team_points)\n",
    "        team_rating_list.append(team_rating)\n",
    "\n",
    "# Create data frame\n",
    "data = {\n",
    "    'Team': team_name_list,\n",
    "    'Matches': team_matches_list,\n",
    "    'Points': team_points_list,\n",
    "    'Rating': team_rating_list\n",
    "}\n",
    "df_teams = pd.DataFrame(data)\n",
    "\n",
    "# Print data frame\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "print(df_teams.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d2034a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Batsman Name Team Rating\n",
      "0             Babar Azam  PAK    887\n",
      "1  Rassie van der Dussen   SA    777\n",
      "2            Imam-ul-Haq  PAK    740\n",
      "3           Shubman Gill  IND    738\n",
      "4           David Warner  AUS    726\n",
      "5            Virat Kohli  IND    719\n",
      "6        Quinton de Kock   SA    718\n",
      "7           Rohit Sharma  IND    707\n",
      "8            Steve Smith  AUS    702\n",
      "9           Fakhar Zaman  PAK    699\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "batsmen = soup.find_all('tr', {'class': ['rankings-block__banner', 'table-body']})[:10]\n",
    "\n",
    "batsmen_name_list = []\n",
    "batsmen_team_list = []\n",
    "batsmen_rating_list = []\n",
    "\n",
    "for batsman in batsmen:\n",
    "    batsman_name = batsman.find('div', {'class': 'rankings-block__banner--name-large'})\n",
    "    if batsman_name is not None:\n",
    "        batsman_name = batsman_name.text.strip()\n",
    "    else:\n",
    "        batsman_name = batsman.find('td', {'class': 'table-body__cell rankings-table__name name'}).text.strip()\n",
    "    batsmen_team = batsman.find('div', {'class': 'rankings-block__banner--nationality'})\n",
    "    if batsmen_team is not None:\n",
    "        batsmen_team = batsmen_team.text.strip()\n",
    "    else:\n",
    "        batsmen_team = batsman.find('span', {'class': 'table-body__logo-text'}).text.strip()\n",
    "    batsmen_rating = batsman.find('div', {'class': 'rankings-block__banner--rating'})\n",
    "    if batsmen_rating is not None:\n",
    "        batsmen_rating = batsmen_rating.text.strip()\n",
    "    else:\n",
    "        batsmen_rating = batsman.find('td', {'class': 'table-body__cell rating'}).text.strip()\n",
    "\n",
    "    batsmen_name_list.append(batsman_name)\n",
    "    batsmen_team_list.append(batsmen_team)\n",
    "    batsmen_rating_list.append(batsmen_rating)\n",
    "\n",
    "data = {\n",
    "    'Batsman Name': batsmen_name_list,\n",
    "    'Team': batsmen_team_list,\n",
    "    'Rating': batsmen_rating_list\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b65acbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Bowler Name Team Rating\n",
      "0    Josh Hazlewood  AUS    705\n",
      "1       Trent Boult   NZ    694\n",
      "2    Mohammed Siraj  IND    691\n",
      "3    Mitchell Starc  AUS    686\n",
      "4        Matt Henry   NZ    676\n",
      "5       Rashid Khan  AFG    659\n",
      "6        Adam Zampa  AUS    652\n",
      "7    Shaheen Afridi  PAK    641\n",
      "8  Mujeeb Ur Rahman  AFG    637\n",
      "9   Shakib Al Hasan  BAN    636\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "bowlers = soup.find_all('tr', {'class': ['rankings-block__banner', 'table-body']})[:10]\n",
    "\n",
    "bowler_name_list = []\n",
    "bowler_team_list = []\n",
    "bowler_rating_list = []\n",
    "\n",
    "for bowler in bowlers:\n",
    "    bowler_name = bowler.find('div', {'class': 'rankings-block__banner--name-large'})\n",
    "    if bowler_name is not None:\n",
    "        bowler_name = bowler_name.text.strip()\n",
    "    else:\n",
    "        bowler_name = bowler.find('td', {'class': 'table-body__cell rankings-table__name name'}).text.strip()\n",
    "    bowler_team = bowler.find('div', {'class': 'rankings-block__banner--nationality'})\n",
    "    if bowler_team is not None:\n",
    "        bowler_team = bowler_team.text.strip()\n",
    "    else:\n",
    "        bowler_team = bowler.find('span', {'class': 'table-body__logo-text'}).text.strip()\n",
    "    bowler_rating = bowler.find('div', {'class': 'rankings-block__banner--rating'})\n",
    "    if bowler_rating is not None:\n",
    "        bowler_rating = bowler_rating.text.strip()\n",
    "    else:\n",
    "        bowler_rating = bowler.find('td', {'class': 'table-body__cell rating'}).text.strip()\n",
    "\n",
    "    bowler_name_list.append(bowler_name)\n",
    "    bowler_team_list.append(bowler_team)\n",
    "    bowler_rating_list.append(bowler_rating)\n",
    "\n",
    "data = {\n",
    "    'Bowler Name': bowler_name_list,\n",
    "    'Team': bowler_team_list,\n",
    "    'Rating': bowler_rating_list\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "686fe77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Team Name Matches Points Rating\n",
      "0     Australia                      \n",
      "1       England      28  3,342       \n",
      "2  South Africa      26  3,098       \n",
      "3         India      27  2,820       \n",
      "4   New Zealand      25  2,553       \n",
      "5   West Indies      27  2,535       \n",
      "6    Bangladesh      13    983       \n",
      "7      Thailand      11    821       \n",
      "8      Pakistan      27  1,678       \n",
      "9     Sri Lanka       8    353       \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "teams = soup.find_all('tr', {'class': ['rankings-block__banner', 'table-body']})[:10]\n",
    "\n",
    "team_name_list = []\n",
    "team_matches_list = []\n",
    "team_points_list = []\n",
    "team_rating_list = []\n",
    "\n",
    "for team in teams:\n",
    "    team_name = team.find('span', {'class': 'u-hide-phablet'})\n",
    "    if team_name is not None:\n",
    "        team_name = team_name.text.strip()\n",
    "    else:\n",
    "        team_name = team.find('td', {'class': 'table-body__cell rankings-table__team'}).text.strip()\n",
    "    team_matches = team.find_all('td', {'class': 'table-body__cell u-center-text'})\n",
    "    if len(team_matches) > 0:\n",
    "        team_matches = team_matches[0].text.strip()\n",
    "    else:\n",
    "        team_matches = ''\n",
    "    team_points = team.find_all('td', {'class': 'table-body__cell u-center-text'})\n",
    "    if len(team_points) > 1:\n",
    "        team_points = team_points[1].text.strip()\n",
    "    else:\n",
    "        team_points = ''\n",
    "    team_rating = team.find('td', {'class': 'table-body__cell rating'})\n",
    "    if team_rating is not None:\n",
    "        team_rating = team_rating.text.strip()\n",
    "    else:\n",
    "        team_rating = ''\n",
    "    team_name_list.append(team_name)\n",
    "    team_matches_list.append(team_matches)\n",
    "    team_points_list.append(team_points)\n",
    "    team_rating_list.append(team_rating)\n",
    "\n",
    "df = pd.DataFrame({'Team Name': team_name_list, 'Matches': team_matches_list, 'Points': team_points_list, 'Rating': team_rating_list})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d53d86df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Name Team Rating\n",
      "0         Alyssa Healy  AUS    762\n",
      "1          Beth Mooney  AUS    754\n",
      "2      Laura Wolvaardt   SA    732\n",
      "3       Natalie Sciver  ENG    731\n",
      "4          Meg Lanning  AUS    717\n",
      "5     Harmanpreet Kaur  IND    716\n",
      "6      Smriti Mandhana  IND    714\n",
      "7  Chamari Athapaththu   SL    655\n",
      "8    Amy Satterthwaite   NZ    641\n",
      "9         Ellyse Perry  AUS    626\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "batsmen = soup.find_all('tr', {'class': ['rankings-block__banner', 'table-body']})[:10]\n",
    "\n",
    "batsman_name_list = []\n",
    "batsman_team_list = []\n",
    "batsman_rating_list = []\n",
    "\n",
    "for batsman in batsmen:\n",
    "    batsman_name = batsman.find('div', {'class': 'rankings-block__banner--name-large'})\n",
    "    if batsman_name is not None:\n",
    "        batsman_name = batsman_name.text.strip()\n",
    "    else:\n",
    "        batsman_name = batsman.find('td', {'class': 'table-body__cell rankings-table__name name'}).text.strip()\n",
    "    batsman_team = batsman.find('div', {'class': 'rankings-block__banner--nationality'})\n",
    "    if batsman_team is not None:\n",
    "        batsman_team = batsman_team.text.strip()\n",
    "    else:\n",
    "        batsman_team = batsman.find('span', {'class': 'table-body__logo-text'}).text.strip()\n",
    "    batsman_rating = batsman.find('div', {'class': 'rankings-block__banner--rating'})\n",
    "    if batsman_rating is not None:\n",
    "        batsman_rating = batsman_rating.text.strip()\n",
    "    else:\n",
    "        batsman_rating = batsman.find('td', {'class': 'table-body__cell rating'}).text.strip()\n",
    "    batsman_name_list.append(batsman_name)\n",
    "    batsman_team_list.append(batsman_team)\n",
    "    batsman_rating_list.append(batsman_rating)\n",
    "\n",
    "batsmen_dict = {'Name': batsman_name_list, 'Team': batsman_team_list, 'Rating': batsman_rating_list}\n",
    "\n",
    "batsmen_df = pd.DataFrame(batsmen_dict)\n",
    "\n",
    "print(batsmen_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66182c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Player Name Team Rating\n",
      "0    Sophie Ecclestone  ENG    751\n",
      "1        Jess Jonassen  AUS    723\n",
      "2       Shabnim Ismail   SA    722\n",
      "3         Megan Schutt  AUS    704\n",
      "4      Hayley Matthews   WI    660\n",
      "5           Kate Cross  ENG    655\n",
      "6       Ayabonga Khaka   SA    634\n",
      "7  Rajeshwari Gayakwad  IND    617\n",
      "8       Marizanne Kapp   SA    598\n",
      "9        Deepti Sharma  IND    589\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "bowlers = soup.find_all('tr', {'class': ['rankings-block__banner', 'table-body']})[:10]\n",
    "\n",
    "bowler_name_list = []\n",
    "bowler_team_list = []\n",
    "bowler_rating_list = []\n",
    "\n",
    "for bowler in bowlers:\n",
    "    bowler_name = bowler.find('div', {'class': 'rankings-block__banner--name-large'})\n",
    "    if bowler_name is not None:\n",
    "        bowler_name = bowler_name.text.strip()\n",
    "    else:\n",
    "        bowler_name = bowler.find('td', {'class': 'table-body__cell rankings-table__name name'}).text.strip()\n",
    "    bowler_team = bowler.find('div', {'class': 'rankings-block__banner--nationality'})\n",
    "    if bowler_team is not None:\n",
    "        bowler_team = bowler_team.text.strip()\n",
    "    else:\n",
    "        bowler_team = bowler.find('span', {'class': 'table-body__logo-text'}).text.strip()\n",
    "    bowler_rating = bowler.find('div', {'class': 'rankings-block__banner--rating'})\n",
    "    if bowler_rating is not None:\n",
    "        bowler_rating = bowler_rating.text.strip()\n",
    "    else:\n",
    "        bowler_rating = bowler.find('td', {'class': 'table-body__cell rating'}).text.strip()\n",
    "    bowler_name_list.append(bowler_name)\n",
    "    bowler_team_list.append(bowler_team)\n",
    "    bowler_rating_list.append(bowler_rating)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Player Name': bowler_name_list,\n",
    "    'Team': bowler_team_list,\n",
    "    'Rating': bowler_rating_list\n",
    "})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "54287a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                               Headline  \\\n",
      "0                   Ukraine should use China as leverage to help win the war with Russia, minister says   \n",
      "1     Russian airstrikes in Ukrainian cities kill at least 23; Kyiv says it's planning counteroffensive   \n",
      "2                                 China just called Ukraine. The timing wasn't accidental, analysts say   \n",
      "3                       Powell duped by Russian pranksters claiming to be Ukraine's Volodymyr Zelenskyy   \n",
      "4   Moscow 'welcomes' China contacting Ukraine; Kyiv says Russia 'won't get away with' Mykolaiv strikes   \n",
      "5       EU agrees to decarbonize air travel with the 'world's largest green fuels mandate for aviation'   \n",
      "6                                  Oliver Stone and Joshua Goldstein on their documentary 'Nuclear Now'   \n",
      "7                       Electric car sales in 2022 hit over 10 million, and China led the way, IEA says   \n",
      "8  Oliver Stone and Joshua Goldstein on why governments are hesitant to make the move to nuclear energy   \n",
      "9                       VC firms create $60 billion-plus climate tech alliance with backing from the UN   \n",
      "\n",
      "  Time  \\\n",
      "0        \n",
      "1        \n",
      "2        \n",
      "3        \n",
      "4        \n",
      "5        \n",
      "6        \n",
      "7        \n",
      "8        \n",
      "9        \n",
      "\n",
      "                                                                                                                                         News Link  \n",
      "0                                          https://www.cnbc.com/2023/04/28/ukraine-should-use-china-as-leverage-to-win-war-with-russia-finmin.html  \n",
      "1                                       https://www.cnbc.com/2023/04/28/ukraine-war-live-updates-latest-news-on-russia-and-the-war-in-ukraine.html  \n",
      "2                                               https://www.cnbc.com/2023/04/28/why-did-china-just-call-ukraine-analysts-share-their-theories.html  \n",
      "3                                       https://www.cnbc.com/2023/04/27/powell-duped-by-russian-pranksters-claiming-to-be-ukraines-zelenskyy-.html  \n",
      "4                                       https://www.cnbc.com/2023/04/27/ukraine-war-live-updates-latest-news-on-russia-and-the-war-in-ukraine.html  \n",
      "5                                              https://www.cnbc.com/2023/04/26/air-travel-eu-agrees-to-the-worlds-largest-green-fuels-mandate.html  \n",
      "6                                    https://www.cnbc.com/video/2023/04/28/oliver-stone-and-joshua-goldstein-on-their-documentary-nuclear-now.html  \n",
      "7                                   https://www.cnbc.com/2023/04/26/electric-car-sales-surged-by-55percent-in-2022-to-hit-over-10-million-iea.html  \n",
      "8  https://www.cnbc.com/video/2023/04/28/oliver-stone-and-joshua-goldstein-on-why-governments-are-hesitant-to-make-the-move-to-nuclear-energy.html  \n",
      "9                                           https://www.cnbc.com/2023/04/25/vc-firms-form-62point3-billion-climate-tech-alliance-backed-by-un.html  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "news_titles = soup.find_all('div', class_='Card-titleContainer')\n",
    "headline_list = []\n",
    "time_list = []\n",
    "link_list = []\n",
    "\n",
    "for news in news_titles:\n",
    "    headline = news.find('a')\n",
    "    if headline:\n",
    "        headline = headline.text.strip()\n",
    "        headline_list.append(headline)\n",
    "    else:\n",
    "        headline_list.append('')\n",
    "\n",
    "    time = news.find('time')\n",
    "    if time:\n",
    "        time = time.text.strip()\n",
    "        time_list.append(time)\n",
    "    else:\n",
    "        time_list.append('')\n",
    "\n",
    "    link = news.find('a')\n",
    "    if link:\n",
    "        link = link['href']\n",
    "        link_list.append(link)\n",
    "    else:\n",
    "        link_list.append('')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Headline': headline_list,\n",
    "    'Time': time_list,\n",
    "    'News Link': link_list\n",
    "})\n",
    "\n",
    "pd.set_option('display.max_colwidth', None) # Display full width of columns\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9494c15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "articles = soup.find_all('div', class_='pod-listing has-cover cf')\n",
    "\n",
    "paper_title_list = []\n",
    "authors_list = []\n",
    "published_date_list = []\n",
    "paper_url_list = []\n",
    "\n",
    "for article in articles:\n",
    "    paper_title = article.find('a', class_='pod-listing__title')\n",
    "    if paper_title:\n",
    "        paper_title = paper_title.text.strip()\n",
    "        paper_title_list.append(paper_title)\n",
    "    else:\n",
    "        paper_title_list.append(None)\n",
    "        \n",
    "    authors = article.find('div', class_='pod-listing__authors')\n",
    "    if authors:\n",
    "        authors = authors.text.strip()\n",
    "        authors_list.append(authors)\n",
    "    else:\n",
    "        authors_list.append(None)\n",
    "        \n",
    "    published_date = article.find('div', class_='pod-listing__meta')\n",
    "    if published_date:\n",
    "        published_date = published_date.text.strip().split('|')[0].strip()\n",
    "        published_date_list.append(published_date)\n",
    "    else:\n",
    "        published_date_list.append(None)\n",
    "    \n",
    "    paper_url = article.find('a', class_='pod-listing__title')\n",
    "    if paper_url:\n",
    "        paper_url = paper_url['href']\n",
    "        paper_url_list.append(paper_url)\n",
    "    else:\n",
    "        paper_url_list.append(None)\n",
    "\n",
    "df = pd.DataFrame({'Paper Title': paper_title_list,\n",
    "                   'Authors': authors_list,\n",
    "                   'Published Date': published_date_list,\n",
    "                   'Paper URL': paper_url_list})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b15b4af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14468\\2298659513.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# loop through the restaurant details and append the data to the lists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrestaurant\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrestaurant_div\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'restnt-info'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrestaurant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'restnt-name'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mrestaurant_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# make a GET request to the website\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants/buffet-special'\n",
    "response = requests.get(url)\n",
    "\n",
    "# create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# find the container for all the restaurant details\n",
    "restaurant_div = soup.find('div', {'id': 'searchList'})\n",
    "\n",
    "# create empty lists to store the restaurant details\n",
    "restaurant_names = []\n",
    "cuisines = []\n",
    "locations = []\n",
    "ratings = []\n",
    "image_urls = []\n",
    "\n",
    "# loop through the restaurant details and append the data to the lists\n",
    "for restaurant in restaurant_div.find_all('div', {'class': 'restnt-info'}):\n",
    "    name = restaurant.find('a', {'class': 'restnt-name'}).text.strip()\n",
    "    restaurant_names.append(name)\n",
    "    \n",
    "    cuisine = restaurant.find('span', {'class': 'double-line-ellipsis'}).text.strip()\n",
    "    cuisines.append(cuisine)\n",
    "    \n",
    "    location = restaurant.find('div', {'class': 'restnt-loc'}).text.strip()\n",
    "    locations.append(location)\n",
    "    \n",
    "    rating = restaurant.find('div', {'class': 'restnt-rating'}).text.strip()\n",
    "    ratings.append(rating)\n",
    "    \n",
    "    image_url = restaurant.find('img', {'class': 'no-img'})['data-original']\n",
    "    image_urls.append(image_url)\n",
    "\n",
    "# create a dictionary of the restaurant details\n",
    "restaurant_dict = {\n",
    "    'Restaurant name': restaurant_names,\n",
    "    'Cuisine': cuisines,\n",
    "    'Location': locations,\n",
    "    'Ratings': ratings,\n",
    "    'Image URL': image_urls\n",
    "}\n",
    "\n",
    "# create a pandas DataFrame from the dictionary\n",
    "df = pd.DataFrame.from_dict(restaurant_dict)\n",
    "\n",
    "# display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ba430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c501b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
